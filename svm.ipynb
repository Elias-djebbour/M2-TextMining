{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./archive/stemmed_cleaned_data.csv\"\n",
    "\n",
    "df = pd.read_csv(path, delimiter=\",\", quotechar='\"')\n",
    "\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©traitement des tags\n",
    "tags = [\n",
    "    \"Blockchain\", \"Data Science\", \"Technology\", \"Programming\", \"Poetry\",\n",
    "    \"Cryptocurrency\", \"Machine Learning\", \"Life\", \"Bitcoin\", \"Writing\",\n",
    "    \"Politics\", \"Startup\", \"Life Lessons\", \"Self Improvement\", \"Covid 19\",\n",
    "    \"Software Development\", \"Love\", \"Python\", \"Business\", \"Health\"\n",
    "]\n",
    "\n",
    "top_20_tags = tags[:20].copy()\n",
    "\n",
    "df['tags'] = df['tags'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "df = df[df['tags'].apply(lambda tags: any(tag in top_20_tags for tag in tags))].copy()\n",
    "\n",
    "df['main_tag'] = df['tags'].apply(lambda tags: next((tag for tag in tags if tag in top_20_tags), None))\n",
    "\n",
    "df = df.dropna(subset=['clean_text'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Transformation TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R√©duction de Dimension avec LDA (LDA n√©cessite y cat√©gorique)\n",
    "y = df['main_tag'].astype('category').cat.codes  # Encodage des classes\n",
    "lda = LDA(n_components=min(len(np.unique(y))-1, 19))  # LDA limit√© √† nb_classes-1\n",
    "X_lda = lda.fit_transform(X_tfidf.toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Entra√Ænement avec C = 0.01\n",
      "‚úî C = 0.01 ‚Üí Accuracy: 0.6283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.44      0.50       535\n",
      "           1       0.62      0.75      0.68      1012\n",
      "           2       0.56      0.56      0.56       548\n",
      "           3       0.70      0.72      0.71       687\n",
      "           4       0.55      0.46      0.50       666\n",
      "           5       0.56      0.60      0.58       799\n",
      "           6       0.79      0.65      0.72       586\n",
      "           7       0.42      0.44      0.43       677\n",
      "           8       0.50      0.48      0.49       744\n",
      "           9       0.61      0.56      0.58       527\n",
      "          10       0.69      0.73      0.71       925\n",
      "          11       0.67      0.83      0.74      1037\n",
      "          12       0.84      0.86      0.85       943\n",
      "          13       0.61      0.73      0.67       864\n",
      "          14       0.59      0.53      0.56       536\n",
      "          15       0.52      0.53      0.52       632\n",
      "          16       0.61      0.43      0.50       541\n",
      "          17       0.70      0.61      0.65       845\n",
      "          18       0.57      0.54      0.56       972\n",
      "          19       0.73      0.69      0.71       854\n",
      "\n",
      "    accuracy                           0.63     14930\n",
      "   macro avg       0.62      0.61      0.61     14930\n",
      "weighted avg       0.63      0.63      0.63     14930\n",
      "\n",
      "\n",
      "üîπ Entra√Ænement avec C = 0.1\n",
      "‚úî C = 0.1 ‚Üí Accuracy: 0.6352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53       535\n",
      "           1       0.63      0.75      0.68      1012\n",
      "           2       0.57      0.57      0.57       548\n",
      "           3       0.68      0.75      0.71       687\n",
      "           4       0.56      0.47      0.51       666\n",
      "           5       0.56      0.57      0.57       799\n",
      "           6       0.76      0.68      0.71       586\n",
      "           7       0.46      0.42      0.44       677\n",
      "           8       0.54      0.47      0.50       744\n",
      "           9       0.59      0.58      0.58       527\n",
      "          10       0.67      0.75      0.71       925\n",
      "          11       0.68      0.82      0.74      1037\n",
      "          12       0.83      0.87      0.85       943\n",
      "          13       0.63      0.72      0.67       864\n",
      "          14       0.57      0.56      0.57       536\n",
      "          15       0.52      0.53      0.52       632\n",
      "          16       0.58      0.46      0.52       541\n",
      "          17       0.71      0.64      0.67       845\n",
      "          18       0.63      0.51      0.57       972\n",
      "          19       0.72      0.72      0.72       854\n",
      "\n",
      "    accuracy                           0.64     14930\n",
      "   macro avg       0.62      0.62      0.62     14930\n",
      "weighted avg       0.63      0.64      0.63     14930\n",
      "\n",
      "\n",
      "üîπ Entra√Ænement avec C = 1\n",
      "‚úî C = 1 ‚Üí Accuracy: 0.6346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.51      0.53       535\n",
      "           1       0.62      0.74      0.68      1012\n",
      "           2       0.57      0.57      0.57       548\n",
      "           3       0.67      0.75      0.71       687\n",
      "           4       0.54      0.46      0.50       666\n",
      "           5       0.57      0.57      0.57       799\n",
      "           6       0.75      0.68      0.71       586\n",
      "           7       0.48      0.40      0.44       677\n",
      "           8       0.53      0.43      0.48       744\n",
      "           9       0.58      0.57      0.57       527\n",
      "          10       0.66      0.76      0.71       925\n",
      "          11       0.67      0.83      0.74      1037\n",
      "          12       0.83      0.87      0.85       943\n",
      "          13       0.63      0.72      0.67       864\n",
      "          14       0.57      0.58      0.57       536\n",
      "          15       0.52      0.54      0.53       632\n",
      "          16       0.58      0.45      0.51       541\n",
      "          17       0.70      0.66      0.68       845\n",
      "          18       0.66      0.51      0.57       972\n",
      "          19       0.72      0.73      0.73       854\n",
      "\n",
      "    accuracy                           0.63     14930\n",
      "   macro avg       0.62      0.62      0.62     14930\n",
      "weighted avg       0.63      0.63      0.63     14930\n",
      "\n",
      "\n",
      "üîπ Entra√Ænement avec C = 10\n",
      "‚úî C = 10 ‚Üí Accuracy: 0.6258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.52      0.53       535\n",
      "           1       0.62      0.73      0.67      1012\n",
      "           2       0.55      0.57      0.56       548\n",
      "           3       0.66      0.73      0.69       687\n",
      "           4       0.55      0.46      0.50       666\n",
      "           5       0.55      0.55      0.55       799\n",
      "           6       0.73      0.67      0.70       586\n",
      "           7       0.48      0.41      0.44       677\n",
      "           8       0.53      0.40      0.46       744\n",
      "           9       0.56      0.53      0.54       527\n",
      "          10       0.66      0.75      0.70       925\n",
      "          11       0.67      0.83      0.74      1037\n",
      "          12       0.83      0.86      0.85       943\n",
      "          13       0.62      0.70      0.66       864\n",
      "          14       0.57      0.58      0.57       536\n",
      "          15       0.51      0.53      0.52       632\n",
      "          16       0.55      0.46      0.50       541\n",
      "          17       0.70      0.65      0.67       845\n",
      "          18       0.66      0.49      0.56       972\n",
      "          19       0.71      0.73      0.72       854\n",
      "\n",
      "    accuracy                           0.63     14930\n",
      "   macro avg       0.61      0.61      0.61     14930\n",
      "weighted avg       0.62      0.63      0.62     14930\n",
      "\n",
      "\n",
      "üîπ Entra√Ænement avec C = 100\n",
      "‚úî C = 100 ‚Üí Accuracy: 0.5853\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       535\n",
      "           1       0.60      0.69      0.64      1012\n",
      "           2       0.48      0.54      0.51       548\n",
      "           3       0.61      0.66      0.63       687\n",
      "           4       0.49      0.40      0.44       666\n",
      "           5       0.51      0.55      0.53       799\n",
      "           6       0.65      0.64      0.64       586\n",
      "           7       0.43      0.41      0.42       677\n",
      "           8       0.45      0.38      0.41       744\n",
      "           9       0.51      0.48      0.49       527\n",
      "          10       0.64      0.72      0.68       925\n",
      "          11       0.65      0.76      0.70      1037\n",
      "          12       0.81      0.81      0.81       943\n",
      "          13       0.59      0.69      0.64       864\n",
      "          14       0.54      0.49      0.51       536\n",
      "          15       0.48      0.48      0.48       632\n",
      "          16       0.49      0.40      0.44       541\n",
      "          17       0.66      0.59      0.62       845\n",
      "          18       0.60      0.46      0.52       972\n",
      "          19       0.68      0.67      0.68       854\n",
      "\n",
      "    accuracy                           0.59     14930\n",
      "   macro avg       0.57      0.57      0.57     14930\n",
      "weighted avg       0.58      0.59      0.58     14930\n",
      "\n",
      "\n",
      "üìå Moyenne de l'accuracy sur toutes les valeurs de C: 0.6218\n",
      "üìä R√©sultats complets : [0.628332217012726, 0.63516409912927, 0.6345612860013395, 0.6257870060281313, 0.5853315472203617]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# %% Division des donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# %% Liste des valeurs de C √† tester\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "results = []\n",
    "\n",
    "# %% Test du SVM pour diff√©rentes valeurs de C\n",
    "for C in C_values:\n",
    "    try:\n",
    "        print(f\"\\nüîπ Entra√Ænement avec C = {C}\", flush=True)  # For√ßage affichage\n",
    "        svm_model = SVC(kernel='rbf', C=C)  \n",
    "        svm_model.fit(X_train, y_train)\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        results.append(acc)  # Stocke la pr√©cision pour chaque C\n",
    "\n",
    "        print(f\"‚úî C = {C} ‚Üí Accuracy: {acc:.4f}\", flush=True)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur avec C = {C} : {e}\", flush=True)\n",
    "\n",
    "# %% Moyenne des pr√©cisions\n",
    "mean_accuracy = np.mean(results)\n",
    "print(f\"\\nüìå Moyenne de l'accuracy sur toutes les valeurs de C: {mean_accuracy:.4f}\")\n",
    "print(f\"üìä R√©sultats complets : {results}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mining_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
